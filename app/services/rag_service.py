from pydantic import BaseModel
import os
import logging
from typing import List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from redisvl.index import SearchIndex
from redisvl.query import VectorQuery
from dotenv import load_dotenv
from langchain_core.tools import StructuredTool
import sys
import numpy as np

root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

from app.models.vector_models import VectorSearchRequest
from app.services.embedding_service import (
    get_embedding_model, 
    get_redis_client, 
    get_redis_url,
    UNIFIED_INDEX_NAME
)

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def standardization(distance: float) -> float:
    """Chuy·ªÉn ƒë·ªïi cosine distance th√†nh similarity score"""
    return 1 - distance

class RAGResponse(BaseModel):
    llm_response: str
    search_type: str = "rag"

class RAGSearchService:
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_API_KEY')
        if self.api_key:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                google_api_key=self.api_key,
                temperature=0.1,
            )
        else:
            self.llm = None

    def search_with_llm(self, request: VectorSearchRequest) -> RAGResponse:
        """
        Search trong unified index
        """
        try:
            embedding_model = get_embedding_model()
            redis_client = get_redis_client()
            redis_url = get_redis_url()
            
            # Ki·ªÉm tra xem c√≥ documents trong unified index kh√¥ng
            pattern = f"doc:{UNIFIED_INDEX_NAME}:*"
            sample_keys = list(redis_client.scan_iter(match=pattern, count=1))
            if not sample_keys:
                logger.warning("Kh√¥ng t√¨m th·∫•y documents trong unified index")
                return RAGResponse(
                    llm_response="Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y.", 
                    search_type="rag"
                )
            
            logger.info(f"T√¨m ki·∫øm trong unified index: {UNIFIED_INDEX_NAME}")
            
            # Generate query embedding
            query_embedding = embedding_model.embed_query(request.query)
            query_vector = np.array(query_embedding, dtype=np.float32)
            
            # T·∫°o VectorQuery cho unified index
            v = VectorQuery(
                vector=query_vector.tolist(),
                vector_field_name="embedding",
                return_fields=["content", "doc_id", "filename", "uploaded_by", "created_at", "chunk_id"],
                num_results=request.k * 2
            )
            
            # Schema cho unified index
            schema = {
                "index": {
                    "name": UNIFIED_INDEX_NAME,
                    "prefix": f"doc:{UNIFIED_INDEX_NAME}",
                    "storage_type": "hash"
                },
                "fields": [
                    {"name": "content", "type": "text"},
                    {"name": "doc_id", "type": "tag"},
                    {"name": "filename", "type": "tag"},
                    {"name": "uploaded_by", "type": "text"},
                    {"name": "created_at", "type": "text"},
                    {"name": "chunk_id", "type": "numeric"},
                    {
                        "name": "embedding",
                        "type": "vector",
                        "attrs": {
                            "dims": len(query_embedding),
                            "distance_metric": "cosine",
                            "algorithm": "flat",
                            "datatype": "float32"
                        }
                    }
                ]
            }
            
            # Connect v√† search
            index = SearchIndex.from_dict(schema)
            index.connect(redis_url)
            results = index.query(v)
            
            # Process results
            all_results = []
            for result in results:
                similarity = standardization(float(result.get('vector_distance', 1.0)))
                if similarity >= request.similarity_threshold:
                    all_results.append({
                        "content": result.get('content', ''),
                        "metadata": {
                            "doc_id": result.get('doc_id', ''),
                            "filename": result.get('filename', ''),
                            "uploaded_by": result.get('uploaded_by', ''),
                            "created_at": result.get('created_at', ''),
                            "chunk_id": result.get('chunk_id', 0),
                            "similarity_score": similarity
                        }
                    })
            
            # Sort v√† take top k
            all_results.sort(key=lambda x: x['metadata']['similarity_score'], reverse=True)
            top_results = all_results[:request.k]
            
            logger.info(f"T√¨m th·∫•y {len(top_results)} k·∫øt qu·∫£ sau khi l·ªçc (ng∆∞·ª°ng: {request.similarity_threshold})")
            
            # Generate LLM response
            llm_response = "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin n√†y."
            if top_results:
                try:
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-2.5-flash",
                        google_api_key=os.getenv('GOOGLE_API_KEY'),
                        temperature=0.3
                    )
                    
                    context = "\n\n".join(
                        [f"T√†i li·ªáu {i+1} (t·ª´ {result['metadata']['filename']}):\n{result['content']}" 
                         for i, result in enumerate(top_results)]
                    )
                    
                    prompt_template = PromptTemplate(
                        input_variables=["query", "context"],
                        template="""
üéØ Vai tr√≤:
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ **t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p**.

üìã Nguy√™n t·∫Øc:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu
- Kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- Kh√¥ng suy ƒëo√°n ho·∫∑c gi·∫£ ƒë·ªãnh
- N·∫øu kh√¥ng c√≥ th√¥ng tin: "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."

üìù C·∫•u tr√∫c tr·∫£ l·ªùi:
1. **C√¢u m·ªü ƒë·∫ßu**: T√≥m t·∫Øt ng·∫Øn g·ªçn (1-2 c√¢u)
2. **N·ªôi dung ch√≠nh**: Tr√¨nh b√†y b·∫±ng danh s√°ch c√≥ s·ªë th·ª© t·ª± ho·∫∑c g·∫°ch ƒë·∫ßu d√≤ng
3. **K·∫øt lu·∫≠n** (n·∫øu c·∫ßn): T√≥m l∆∞·ª£c ho·∫∑c l·ªùi khuy√™n

üí° Format markdown:
- D√πng **s·ªë th·ª© t·ª±** (1., 2., 3.) cho c√°c b∆∞·ªõc ho·∫∑c quy tr√¨nh
- D√πng **g·∫°ch ƒë·∫ßu d√≤ng** (-, *, ‚Ä¢) cho danh s√°ch c√°c √Ω
- D√πng **bold** cho t·ª´ kh√≥a quan tr·ªçng
- D√πng > cho tr√≠ch d·∫´n t·ª´ t√†i li·ªáu (n·∫øu c·∫ßn)

‚ùì C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
{query}

üìÇ T√†i li·ªáu tham kh·∫£o:
{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu tr√™n.
"""
                    )
                    
                    prompt = prompt_template.format(query=request.query, context=context)
                    llm_response = llm.invoke(prompt).content
                    
                    logger.info(f"ƒê√£ t·∫°o LLM response th√†nh c√¥ng (s·ª≠ d·ª•ng {len(top_results)} documents)")
                    
                except Exception as e:
                    logger.error(f"L·ªói t·∫°o LLM response: {str(e)}")
                    llm_response = "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM."
            
            return RAGResponse(llm_response=llm_response, search_type="rag")
            
        except Exception as e:
            logger.error(f"L·ªói kh√¥ng mong ƒë·ª£i trong RAG search: {str(e)}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            return RAGResponse(llm_response="L·ªói h·ªá th·ªëng.", search_type="rag")


# Initialize service
rag_service = RAGSearchService()


# WRAPPER FUNCTION CHO RAG

def rag_search_wrapper(query, k=5, similarity_threshold=0.5):
    """Wrapper tr·∫£ v·ªÅ dict cho RAG search"""
    logger.info(f"[WRAPPER] rag_search ƒë∆∞·ª£c g·ªçi v·ªõi query: {query[:50]}...")
    
    result = rag_service.search_with_llm(
        VectorSearchRequest(
            query=query,
            k=k,
            similarity_threshold=similarity_threshold
        )
    )
    
    # Tr·∫£ v·ªÅ dict structured
    output = {
        "llm_response": result.llm_response,
        "source": "rag",
        "search_type": "rag",
        "activities_raw": [],  # RAG kh√¥ng c√≥ activities
        "total": 0
    }
    
    logger.info("[WRAPPER] Tr·∫£ v·ªÅ RAG response (kh√¥ng c√≥ activities)")
    
    return output


# LANGCHAIN TOOL - S·ª¨ D·ª§NG WRAPPER

rag_search_tool = StructuredTool.from_function(
    func=rag_search_wrapper,
    name="vector_rag_search",
    description=f"""
Th·ª±c hi·ªán RAG search tr√™n unified Redis vector database ({UNIFIED_INDEX_NAME}) ƒë·ªÉ t√¨m t√†i li·ªáu t∆∞∆°ng t·ª± v√† generate c√¢u tr·∫£ l·ªùi t·ª´ LLM (Gemini).

Input parameters:
- query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng
- k (int, default=5): S·ªë l∆∞·ª£ng documents c·∫ßn l·∫•y
- similarity_threshold (float, default=0.5): Ng∆∞·ª°ng similarity t·ªëi thi·ªÉu (0-1)

Output: 
- llm_response: C√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c generate t·ª´ LLM
- source: "rag"
- search_type: "rag"
- activities_raw: [] (RAG kh√¥ng tr·∫£ v·ªÅ ho·∫°t ƒë·ªông)

Index hi·ªán t·∫°i: {UNIFIED_INDEX_NAME} (ch·ª©a t·∫•t c·∫£ documents c·ªßa h·ªá th·ªëng)
"""
)