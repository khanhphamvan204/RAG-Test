from pydantic import BaseModel
import os
import logging
from typing import List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from redisvl.index import SearchIndex
from redisvl.query import VectorQuery
from dotenv import load_dotenv
from langchain_core.tools import StructuredTool
import numpy as np

from app.models.vector_models import VectorSearchRequest
from app.services.embedding_service import (
    get_embedding_model, 
    get_redis_client, 
    get_redis_url
)
from app.config import Config

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def standardization(distance: float) -> float:
    """Chuyá»ƒn Ä‘á»•i cosine distance thÃ nh similarity score"""
    return 1 - distance

class RAGResponse(BaseModel):
    llm_response: str
    search_type: str = "rag"
    unit_name: str = ""

class RAGSearchService:
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_API_KEY')
        if self.api_key:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                google_api_key=self.api_key,
                temperature=0.1,
            )
        else:
            self.llm = None

    def search_with_llm(self, request: VectorSearchRequest, unit_name: str = "default_unit") -> RAGResponse:
        """
        Search trong unit-specific index
        """
        try:
            embedding_model = get_embedding_model()
            redis_client = get_redis_client()
            redis_url = get_redis_url()
            
            # Láº¥y index name cá»§a unit
            index_name = Config.get_unit_index_name(unit_name)
            
            logger.info(f"[RAG] Searching in unit index: {index_name}")
            
            # Kiá»ƒm tra documents trong unit index
            pattern = f"doc:{index_name}:*"
            sample_keys = list(redis_client.scan_iter(match=pattern, count=1))
            
            if not sample_keys:
                logger.warning(f"No documents found in unit index: {index_name}")
                return RAGResponse(
                    llm_response=f"Xin lá»—i, khÃ´ng tÃ¬m tháº¥y tÃ i liá»‡u nÃ o trong thÆ° viá»‡n cá»§a Ä‘Æ¡n vá»‹ '{unit_name}'.", 
                    search_type="rag",
                    unit_name=unit_name
                )
            
            # Generate query embedding
            query_embedding = embedding_model.embed_query(request.query)
            query_vector = np.array(query_embedding, dtype=np.float32)
            
            # Vector query
            v = VectorQuery(
                vector=query_vector.tolist(),
                vector_field_name="embedding",
                return_fields=["content", "doc_id", "filename", "uploaded_by", "created_at", "chunk_id", "unit_name"],
                num_results=request.k * 2
            )
            
            # Schema cho unit index
            schema = {
                "index": {
                    "name": index_name,
                    "prefix": f"doc:{index_name}",
                    "storage_type": "hash"
                },
                "fields": [
                    {"name": "content", "type": "text"},
                    {"name": "doc_id", "type": "tag"},
                    {"name": "filename", "type": "tag"},
                    {"name": "uploaded_by", "type": "text"},
                    {"name": "created_at", "type": "text"},
                    {"name": "chunk_id", "type": "numeric"},
                    {"name": "unit_name", "type": "tag"},
                    {
                        "name": "embedding",
                        "type": "vector",
                        "attrs": {
                            "dims": len(query_embedding),
                            "distance_metric": "cosine",
                            "algorithm": "flat",
                            "datatype": "float32"
                        }
                    }
                ]
            }
            
            # Connect vÃ  search
            index = SearchIndex.from_dict(schema)
            index.connect(redis_url)
            results = index.query(v)
            
            # Process results
            all_results = []
            for result in results:
                similarity = standardization(float(result.get('vector_distance', 1.0)))
                if similarity >= request.similarity_threshold:
                    all_results.append({
                        "content": result.get('content', ''),
                        "metadata": {
                            "doc_id": result.get('doc_id', ''),
                            "filename": result.get('filename', ''),
                            "uploaded_by": result.get('uploaded_by', ''),
                            "created_at": result.get('created_at', ''),
                            "chunk_id": result.get('chunk_id', 0),
                            "similarity_score": similarity,
                            "unit_name": result.get('unit_name', unit_name)
                        }
                    })
            
            # Sort vÃ  take top k
            all_results.sort(key=lambda x: x['metadata']['similarity_score'], reverse=True)
            top_results = all_results[:request.k]
            
            logger.info(f"[RAG] Found {len(top_results)} results in unit '{unit_name}'")
            
            # Generate LLM response
            llm_response = f"Xin lá»—i, khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong thÆ° viá»‡n cá»§a Ä‘Æ¡n vá»‹ '{unit_name}'."
            
            if top_results:
                try:
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-2.5-flash",
                        google_api_key=os.getenv('GOOGLE_API_KEY'),
                        temperature=0.3
                    )
                    
                    context = "\n\n".join(
                        [f"TÃ i liá»‡u {i+1} (tá»« {result['metadata']['filename']}):\n{result['content']}" 
                         for i, result in enumerate(top_results)]
                    )
                    
                    prompt_template = PromptTemplate(
                        input_variables=["query", "context", "unit_name"],
                        template="""
ğŸ¯ Vai trÃ²:
Báº¡n lÃ  trá»£ lÃ½ AI cá»§a thÆ° viá»‡n Ä‘Æ¡n vá»‹ "{unit_name}", chá»‰ tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p.

ğŸ“‹ NguyÃªn táº¯c:
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u cá»§a Ä‘Æ¡n vá»‹ "{unit_name}"
- KhÃ´ng thÃªm kiáº¿n thá»©c bÃªn ngoÃ i
- KhÃ´ng suy Ä‘oÃ¡n hoáº·c giáº£ Ä‘á»‹nh
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u."

ğŸ“ Cáº¥u trÃºc tráº£ lá»i:
1. **CÃ¢u má»Ÿ Ä‘áº§u**: TÃ³m táº¯t ngáº¯n gá»n (1-2 cÃ¢u)
2. **Ná»™i dung chÃ­nh**: TrÃ¬nh bÃ y báº±ng danh sÃ¡ch cÃ³ sá»‘ thá»© tá»± hoáº·c gáº¡ch Ä‘áº§u dÃ²ng
3. **Káº¿t luáº­n** (náº¿u cáº§n): TÃ³m lÆ°á»£c hoáº·c lá»i khuyÃªn

ğŸ’¡ Format markdown:
- DÃ¹ng **sá»‘ thá»© tá»±** (1., 2., 3.) cho cÃ¡c bÆ°á»›c hoáº·c quy trÃ¬nh
- DÃ¹ng **gáº¡ch Ä‘áº§u dÃ²ng** (-, *, â€¢) cho danh sÃ¡ch cÃ¡c Ã½
- DÃ¹ng **bold** cho tá»« khÃ³a quan trá»ng

â“ CÃ¢u há»i:
{query}

ğŸ“‚ TÃ i liá»‡u tá»« thÆ° viá»‡n Ä‘Æ¡n vá»‹ "{unit_name}":
{context}

HÃ£y tráº£ lá»i dá»±a trÃªn tÃ i liá»‡u trÃªn.
"""
                    )
                    
                    prompt = prompt_template.format(
                        query=request.query, 
                        context=context,
                        unit_name=unit_name
                    )
                    llm_response = llm.invoke(prompt).content
                    
                    logger.info(f"[RAG] Generated response using {len(top_results)} documents from unit '{unit_name}'")
                    
                except Exception as e:
                    logger.error(f"[RAG] LLM error: {str(e)}")
                    llm_response = "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i tá»« LLM."
            
            return RAGResponse(
                llm_response=llm_response, 
                search_type="rag",
                unit_name=unit_name
            )
            
        except Exception as e:
            logger.error(f"[RAG] Error: {str(e)}")
            import traceback
            logger.error(traceback.format_exc())
            return RAGResponse(
                llm_response="Lá»—i há»‡ thá»‘ng.", 
                search_type="rag",
                unit_name=unit_name
            )


# Initialize service
rag_service = RAGSearchService()


# WRAPPER FUNCTION CHO RAG vá»›i unit support

def rag_search_wrapper(query, unit_name=None, k=5, similarity_threshold=0.3):
    """
    Wrapper tráº£ vá» dict cho RAG search vá»›i unit support
    
    Args:
        query: CÃ¢u há»i
        unit_name: TÃªn Ä‘Æ¡n vá»‹ (náº¿u None thÃ¬ láº¥y tá»« langgraph_service)
        k: Sá»‘ lÆ°á»£ng results
        similarity_threshold: NgÆ°á»¡ng similarity
    """
    # Náº¿u khÃ´ng truyá»n unit_name, láº¥y tá»« langgraph_service
    if unit_name is None:
        try:
            from app.services.langgraph_service import get_current_unit_name
            unit_name = get_current_unit_name()
            logger.info(f"[RAG_WRAPPER] Got unit_name from context: {unit_name}")
        except Exception as e:
            logger.warning(f"[RAG_WRAPPER] Cannot get unit_name from context: {e}, using default")
            unit_name = "default_unit"
    
    logger.info(f"[RAG_WRAPPER] Query in unit '{unit_name}': {query[:50]}...")
    
    result = rag_service.search_with_llm(
        VectorSearchRequest(
            query=query,
            k=k,
            similarity_threshold=similarity_threshold
        ),
        unit_name=unit_name
    )
    
    # Tráº£ vá» dict structured
    output = {
        "llm_response": result.llm_response,
        "source": "rag",
        "search_type": "rag",
        "unit_name": unit_name,
        "activities_raw": [],  # RAG khÃ´ng cÃ³ activities
        "total": 0
    }
    
    logger.info(f"[RAG_WRAPPER] Response from unit '{unit_name}'")
    
    return output


# LANGCHAIN TOOL - vá»›i unit support

rag_search_tool = StructuredTool.from_function(
    func=rag_search_wrapper,
    name="vector_rag_search",
    description="""
Thá»±c hiá»‡n RAG search trÃªn Redis vector database theo ÄÆ N Vá»Š (unit-based) Ä‘á»ƒ tÃ¬m tÃ i liá»‡u vÃ  generate cÃ¢u tráº£ lá»i.

**QUAN TRá»ŒNG**: Má»—i Ä‘Æ¡n vá»‹ cÃ³ thÆ° viá»‡n riÃªng, chá»‰ search trong thÆ° viá»‡n cá»§a Ä‘Æ¡n vá»‹ hiá»‡n táº¡i.

**CÃCH Sá»¬ Dá»¤NG**:
- Náº¿u KHÃ”NG truyá»n unit_name: Tool sáº½ tá»± Ä‘á»™ng láº¥y unit_name tá»« context (ngÆ°á»i dÃ¹ng hiá»‡n táº¡i)
- Náº¿u CÃ“ truyá»n unit_name: Tool sáº½ search trong unit Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh (chá»‰ admin má»›i Ä‘Æ°á»£c dÃ¹ng)

Input parameters:
- query (str, REQUIRED): CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng
- unit_name (str, OPTIONAL): TÃªn Ä‘Æ¡n vá»‹ - Náº¾U KHÃ”NG TRUYá»€N thÃ¬ dÃ¹ng unit cá»§a user hiá»‡n táº¡i
- k (int, default=5): Sá»‘ lÆ°á»£ng documents
- similarity_threshold (float, default=0.3): NgÆ°á»¡ng similarity (0-1)

Output: 
- llm_response: CÃ¢u tráº£ lá»i tá»« LLM dá»±a trÃªn tÃ i liá»‡u
- source: "rag"
- unit_name: TÃªn Ä‘Æ¡n vá»‹ Ä‘Ã£ search
- activities_raw: [] (RAG khÃ´ng tráº£ vá» hoáº¡t Ä‘á»™ng)

**LÆ¯U Ã**: 
- Tool tá»± Ä‘á»™ng láº¥y unit_name tá»« context, KHÃ”NG Cáº¦N truyá»n thá»§ cÃ´ng
- Chá»‰ search trong thÆ° viá»‡n cá»§a Ä‘Æ¡n vá»‹ Ä‘Æ°á»£c chá»‰ Ä‘á»‹nh
- VÃ­ dá»¥ gá»i tool: vector_rag_search(query="quy Ä‘á»‹nh há»c vá»¥", k=5)
"""
)