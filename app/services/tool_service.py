from pydantic import BaseModel
import os
import logging
from typing import List
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from redisvl.index import SearchIndex
from redisvl.query import VectorQuery
from dotenv import load_dotenv
from langchain_core.tools import StructuredTool
import sys
import numpy as np

root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
if root_dir not in sys.path:
    sys.path.insert(0, root_dir)

from app.models.vector_models import VectorSearchRequest
from app.services.embedding_service import get_embedding_model, get_redis_client, get_redis_url

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def standardization(distance: float) -> float:
    """Chuyá»ƒn Ä‘á»•i cosine distance thÃ nh similarity score"""
    return 1 - distance

class RAGResponse(BaseModel):
    llm_response: str
    search_type: str = "rag"

class RAGSearchService:
    def __init__(self):
        self.api_key = os.getenv('GOOGLE_API_KEY')
        if self.api_key:
            self.llm = ChatGoogleGenerativeAI(
                model="gemini-2.5-flash",
                google_api_key=self.api_key,
                temperature=0.1,
            )
        else:
            self.llm = None

    def search_with_llm(self, request: VectorSearchRequest) -> RAGResponse:
        try:
            # Get embedding model
            embedding_model = get_embedding_model()
            redis_client = get_redis_client()
            redis_url = get_redis_url()  # Get Redis URL for SearchIndex
            
            # Get all index names
            all_keys = list(redis_client.scan_iter(match="doc:docs_*:0"))
            if not all_keys:
                logger.warning("No documents found in Redis")
                return RAGResponse(llm_response="Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y.", search_type="rag")
            
            # Extract unique index names
            index_names = set()
            for key in all_keys:
                key_str = key.decode('utf-8') if isinstance(key, bytes) else key
                parts = key_str.split(':')
                if len(parts) >= 2:
                    index_names.add(parts[1])
            
            logger.info(f"Found {len(index_names)} indexes: {index_names}")
            
            # Generate query embedding
            query_embedding = embedding_model.embed_query(request.query)
            query_vector = np.array(query_embedding, dtype=np.float32)
            
            all_results = []
            
            # Search across all indexes
            for index_name in index_names:
                try:
                    # Create VectorQuery
                    v = VectorQuery(
                        vector=query_vector.tolist(),
                        vector_field_name="embedding",
                        return_fields=["content", "doc_id", "filename", "uploaded_by", "created_at"],
                        num_results=request.k
                    )
                    
                    # Get index
                    schema = {
                        "index": {
                            "name": index_name,
                            "prefix": f"doc:{index_name}",
                            "storage_type": "hash"
                        },
                        "fields": [
                            {"name": "content", "type": "text"},
                            {"name": "doc_id", "type": "tag"},
                            {"name": "filename", "type": "text"},
                            {"name": "uploaded_by", "type": "text"},
                            {"name": "created_at", "type": "text"},
                            {
                                "name": "embedding",
                                "type": "vector",
                                "attrs": {
                                    "dims": len(query_embedding),
                                    "distance_metric": "cosine",
                                    "algorithm": "flat",
                                    "datatype": "float32"
                                }
                            }
                        ]
                    }
                    
                    index = SearchIndex.from_dict(schema)
                    index.connect(redis_url)  # Use Redis URL, not client object
                    
                    # Execute search
                    results = index.query(v)
                    
                    for result in results:
                        similarity = standardization(float(result.get('vector_distance', 1.0)))
                        if similarity >= request.similarity_threshold:
                            all_results.append({
                                "content": result.get('content', ''),
                                "metadata": {
                                    "doc_id": result.get('doc_id', ''),
                                    "filename": result.get('filename', ''),
                                    "uploaded_by": result.get('uploaded_by', ''),
                                    "created_at": result.get('created_at', ''),
                                    "similarity_score": similarity
                                }
                            })
                    
                except Exception as e:
                    logger.error(f"Error searching index {index_name}: {e}")
                    continue
            
            # Sort by similarity and take top k
            all_results.sort(key=lambda x: x['metadata']['similarity_score'], reverse=True)
            top_results = all_results[:request.k]
            
            logger.info(f"Found {len(top_results)} results after filtering")
            
            # Generate LLM response
            llm_response = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin nÃ y."
            if top_results:
                try:
                    llm = ChatGoogleGenerativeAI(
                        model="gemini-2.5-flash",
                        google_api_key=os.getenv('GOOGLE_API_KEY'),
                        temperature=0.3
                    )
                    
                    context = "\n\n".join(
                        [f"Document {i+1}:\n{result['content']}" for i, result in enumerate(top_results)]
                    )
                    
                    prompt_template = PromptTemplate(
                        input_variables=["query", "context"],
                        template="""
ğŸ¯ Vai trÃ²:
Báº¡n lÃ  má»™t trá»£ lÃ½ AI chuyÃªn nghiá»‡p, chá»‰ tráº£ lá»i dá»±a trÃªn thÃ´ng tin tá»« **tÃ i liá»‡u Ä‘Æ°á»£c cung cáº¥p**.

ğŸ“‹ NguyÃªn táº¯c:
- Chá»‰ sá»­ dá»¥ng thÃ´ng tin tá»« tÃ i liá»‡u
- KhÃ´ng thÃªm kiáº¿n thá»©c bÃªn ngoÃ i
- KhÃ´ng suy Ä‘oÃ¡n hoáº·c giáº£ Ä‘á»‹nh
- Náº¿u khÃ´ng cÃ³ thÃ´ng tin: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u."

ğŸ“ Cáº¥u trÃºc tráº£ lá»i:
1. **CÃ¢u má»Ÿ Ä‘áº§u**: TÃ³m táº¯t ngáº¯n gá»n (1-2 cÃ¢u)
2. **Ná»™i dung chÃ­nh**: TrÃ¬nh bÃ y báº±ng danh sÃ¡ch cÃ³ sá»‘ thá»© tá»± hoáº·c gáº¡ch Ä‘áº§u dÃ²ng
3. **Káº¿t luáº­n** (náº¿u cáº§n): TÃ³m lÆ°á»£c hoáº·c lá»i khuyÃªn

ğŸ’¡ Format markdown:
- DÃ¹ng **sá»‘ thá»© tá»±** (1., 2., 3.) cho cÃ¡c bÆ°á»›c hoáº·c quy trÃ¬nh
- DÃ¹ng **gáº¡ch Ä‘áº§u dÃ²ng** (-, *, â€¢) cho danh sÃ¡ch cÃ¡c Ã½
- DÃ¹ng **bold** cho tá»« khÃ³a quan trá»ng
- DÃ¹ng > cho trÃ­ch dáº«n tá»« tÃ i liá»‡u (náº¿u cáº§n)

â“ CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng:
{query}

ğŸ“‚ TÃ i liá»‡u tham kháº£o:
{context}

HÃ£y tráº£ lá»i cÃ¢u há»i dá»±a trÃªn tÃ i liá»‡u trÃªn.
"""
                    )
                    
                    prompt = prompt_template.format(query=request.query, context=context)
                    llm_response = llm.invoke(prompt).content
                    
                except Exception as e:
                    logger.error(f"LLM response generation failed: {str(e)}")
                    llm_response = "KhÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i tá»« LLM."
            
            return RAGResponse(llm_response=llm_response, search_type="rag")
            
        except Exception as e:
            logger.error(f"Unexpected error: {str(e)}")
            return RAGResponse(llm_response="Lá»—i há»‡ thá»‘ng.", search_type="rag")

# Initialize service
rag_service = RAGSearchService()

# Define the RAG search tool for LangGraph
rag_search_tool = StructuredTool.from_function(
    func=lambda request: rag_service.search_with_llm(VectorSearchRequest(**request)) if isinstance(request, dict) else rag_service.search_with_llm(request),
    name="vector_rag_search",
    description="Thá»±c hiá»‡n RAG search trÃªn Redis vector database Ä‘á»ƒ tÃ¬m tÃ i liá»‡u tÆ°Æ¡ng tá»± vÃ  generate cÃ¢u tráº£ lá»i tá»« LLM (Gemini). Input lÃ  query vÄƒn báº£n, k (top results), similarity_threshold. Tráº£ vá» llm_response vá»›i cÃ¢u tráº£ lá»i dá»±a trÃªn context tá»« tÃ i liá»‡u."
)