# ==================== FILE: app/services/langgraph_service.py ====================

from typing import TypedDict, Annotated
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langgraph.checkpoint.memory import MemorySaver
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, ToolMessage
import operator
import logging
import json
from dotenv import load_dotenv
import os
from datetime import datetime

from app.services.rag_service import rag_search_tool
from app.services.activity_search_service import (
    activity_search_tool,
    activity_search_with_llm_tool,
    set_bearer_token  # â† IMPORT FUNCTION SET TOKEN
)

load_dotenv()

logger = logging.getLogger(__name__)


class AgentState(TypedDict):
    messages: Annotated[list, operator.add]
    user_role: str
    user_id: int
    bearer_token: str  # Váº«n giá»¯ trong state Ä‘á»ƒ tracking


def create_agent_node(llm_with_tools):
    def agent(state: AgentState):
        messages = state["messages"]
        user_role = state.get("user_role", "student")
        user_id = state.get("user_id", 0)
        
        # Láº¥y ngÃ y giá» hiá»‡n táº¡i
        current_datetime = datetime.now()
        current_date_str = current_datetime.strftime("%d/%m/%Y")
        current_time_str = current_datetime.strftime("%H:%M:%S")
        current_weekday = ["Thá»© Hai", "Thá»© Ba", "Thá»© TÆ°", "Thá»© NÄƒm", "Thá»© SÃ¡u", "Thá»© Báº£y", "Chá»§ Nháº­t"][current_datetime.weekday()]
        
        # ÄÃƒ Bá»Ž HOÃ€N TOÃ€N instruction vá» bearer_token
        system_context = f"""
Báº¡n lÃ  trá»£ lÃ½ AI cho há»‡ thá»‘ng quáº£n lÃ½ cá»‘ váº¥n há»c táº­p.

THÃ”NG TIN THá»œI GIAN HIá»†N Táº I:
- NgÃ y hiá»‡n táº¡i: {current_weekday}, {current_date_str}
- Giá» hiá»‡n táº¡i: {current_time_str}

NgÆ°á»i dÃ¹ng hiá»‡n táº¡i:
- Vai trÃ²: {user_role}
- ID: {user_id}

CÃ´ng cá»¥ cÃ³ sáºµn:
1. **vector_rag_search** - TÃ¬m kiáº¿m tÃ i liá»‡u trong há»‡ thá»‘ng
2. **activity_search** - TÃ¬m kiáº¿m hoáº¡t Ä‘á»™ng ngoáº¡i khÃ³a (dá»¯ liá»‡u thÃ´)
3. **activity_search_with_summary** - TÃ¬m kiáº¿m hoáº¡t Ä‘á»™ng + tÃ³m táº¯t LLM

HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG TOOLS:

ðŸ“š **vector_rag_search**: DÃ¹ng khi user há»i vá»:
- Quy Ä‘á»‹nh, quy trÃ¬nh, ná»™i quy
- TÃ i liá»‡u hÆ°á»›ng dáº«n
- ThÃ´ng tin chung vá» há»‡ thá»‘ng

ðŸŽ¯ **activity_search**: DÃ¹ng khi cáº§n dá»¯ liá»‡u thÃ´ vá» hoáº¡t Ä‘á»™ng:
- Liá»‡t kÃª táº¥t cáº£ hoáº¡t Ä‘á»™ng
- Export/bÃ¡o cÃ¡o
- Xá»­ lÃ½ dá»¯ liá»‡u phá»©c táº¡p

âœ¨ **activity_search_with_summary**: DÃ¹ng khi user há»i vá» hoáº¡t Ä‘á»™ng:
- "CÃ³ hoáº¡t Ä‘á»™ng gÃ¬ sáº¯p tá»›i?"
- "TÃ¬m hoáº¡t Ä‘á»™ng CTXH"
- "Hoáº¡t Ä‘á»™ng nÃ o cho Ä‘iá»ƒm rÃ¨n luyá»‡n?"

CÃCH Gá»ŒI TOOL ACTIVITY:
activity_search_with_summary(
    user_role="{user_role}",
    user_id={user_id},
    status="upcoming"  # hoáº·c cÃ¡c filter khÃ¡c
)

LÆ¯U Ã QUAN TRá»ŒNG:
- KHÃ”NG BAO GIá»œ truyá»n bearer_token vÃ o tool call (há»‡ thá»‘ng tá»± Ä‘á»™ng xá»­ lÃ½)
- Chá»‰ gá»i Má»˜T TOOL activity duy nháº¥t cho má»—i cÃ¢u há»i
- Náº¿u tool tráº£ vá» total=0, Dá»ªNG vÃ  tráº£ lá»i "KhÃ´ng cÃ³ hoáº¡t Ä‘á»™ng phÃ¹ há»£p"
- KHÃ”NG suy Ä‘oÃ¡n hoáº·c tá»± táº¡o dá»¯ liá»‡u hoáº¡t Ä‘á»™ng
"""
        
        full_messages = [SystemMessage(content=system_context)] + messages
        response = llm_with_tools.invoke(full_messages)
        
        return {"messages": [response]}
    
    return agent


def should_continue(state: AgentState):
    messages = state["messages"]
    last_message = messages[-1]
    
    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:
        return "tools"
    
    return END


def create_langgraph():
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        google_api_key=os.getenv('GOOGLE_API_KEY'),
        temperature=0.3
    )
    
    tools = [
        rag_search_tool,
        activity_search_tool,
        activity_search_with_llm_tool
    ]
    
    llm_with_tools = llm.bind_tools(tools)
    
    workflow = StateGraph(AgentState)
    
    workflow.add_node("agent", create_agent_node(llm_with_tools))
    workflow.add_node("tools", ToolNode(tools))
    
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {"tools": "tools", END: END}
    )
    workflow.add_edge("tools", "agent")
    
    memory = MemorySaver()
    return workflow.compile(checkpointer=memory)


graph = create_langgraph()


def process_query(
    query: str,
    user_role: str = "student",
    user_id: int = 0,
    bearer_token: str = None,
    thread_id: str | None = None
) -> str:
    """
    Process query - Tá»° Äá»˜NG INJECT TOKEN trÆ°á»›c khi tools Ä‘Æ°á»£c gá»i
    """
    try:
        # SET TOKEN GLOBAL NGAY Tá»ª Äáº¦U
        if bearer_token:
            set_bearer_token(bearer_token)
            logger.info(f"[PROCESS] Token Ä‘Ã£ Ä‘Æ°á»£c set global: {bearer_token[:30]}...")
        else:
            logger.warning("[PROCESS] KhÃ´ng cÃ³ bearer token - tools sáº½ fail!")
        
        config = {"configurable": {"thread_id": thread_id or "default"}}
        
        initial_state = {
            "messages": [HumanMessage(content=query)],
            "user_role": user_role,
            "user_id": user_id,
            "bearer_token": bearer_token or ""  # Váº«n lÆ°u Ä‘á»ƒ tracking
        }
        
        result = graph.invoke(initial_state, config)
        
        messages = result.get("messages", [])
        if not messages:
            return json.dumps({
                "status": "error",
                "data": None,
                "error": "KhÃ´ng cÃ³ pháº£n há»“i",
                "thread_id": thread_id
            }, ensure_ascii=False, indent=2)
        
        last_message = messages[-1]
        
        # TRÃCH XUáº¤T TEXT PHáº¢N Há»’I
        response_text = last_message.content if hasattr(last_message, 'content') else str(last_message)
        
        # TRÃCH XUáº¤T Káº¾T QUáº¢ TOOL
        activities_raw = []
        source = "general"
        total_activities = 0
        
        tool_messages = [msg for msg in messages if isinstance(msg, ToolMessage)]
        logger.info(f"[DEBUG] TÃ¬m tháº¥y {len(tool_messages)} tool messages")
        
        if tool_messages:
            last_tool_msg = tool_messages[-1]
            logger.info(f"[DEBUG] Tool message cuá»‘i: {last_tool_msg.name if hasattr(last_tool_msg, 'name') else 'unknown'}")
            
            try:
                tool_result = json.loads(last_tool_msg.content) if isinstance(last_tool_msg.content, str) else last_tool_msg.content
                
                if isinstance(tool_result, dict):
                    if tool_result.get('source') == 'activity':
                        activities_raw = tool_result.get('activities_raw', [])
                        total_activities = tool_result.get('total', 0)
                        source = 'activity'
                        logger.info(f"[EXTRACT] TÃ¬m tháº¥y {total_activities} hoáº¡t Ä‘á»™ng")
                    elif tool_result.get('source') == 'rag':
                        source = 'rag'
                        logger.info("[EXTRACT] Nguá»“n lÃ  RAG")
            except Exception as e:
                logger.error(f"[EXTRACT] Lá»—i parse tool message: {e}")
        
        return json.dumps({
            "status": "success",
            "data": {
                "response": response_text,
                "user_role": user_role,
                "user_id": user_id,
                "source": source,
                "activities": activities_raw,
                "total_activities": total_activities
            },
            "error": None,
            "thread_id": thread_id
        }, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"[PROCESS] Lá»—i: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        
        return json.dumps({
            "status": "error",
            "data": None,
            "error": str(e),
            "thread_id": thread_id
        }, ensure_ascii=False, indent=2)


def get_conversation_history(thread_id: str) -> list:
    try:
        config = {"configurable": {"thread_id": thread_id}}
        state = graph.get_state(config)
        return state.values.get("messages", [])
    except Exception as e:
        logger.error(f"Lá»—i khi láº¥y lá»‹ch sá»­ há»™i thoáº¡i: {e}")
        return []


def clear_conversation_history(thread_id: str) -> bool:
    try:
        logger.warning("XÃ³a lá»‹ch sá»­ há»™i thoáº¡i chÆ°a Ä‘Æ°á»£c implement cho MemorySaver")
        return False
    except Exception as e:
        logger.error(f"Lá»—i khi xÃ³a lá»‹ch sá»­ há»™i thoáº¡i: {e}")
        return False