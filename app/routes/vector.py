import traceback
from fastapi import APIRouter, UploadFile, File, HTTPException, Form, Depends
from app.services.embedding_service import add_to_embedding, delete_from_faiss_index, smart_metadata_update
from app.services.langgraph_service import process_query
from app.services.metadata_service import save_metadata, delete_metadata, find_document_info
from app.services.file_service import get_file_paths
from app.services.auth_service import verify_token_v2
from pydantic import BaseModel
from langchain.prompts import PromptTemplate
import os
import json
import uuid
from datetime import datetime, timezone, timedelta
import shutil
import logging
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
import time
from app.services.embedding_service import get_embedding_model
from dotenv import load_dotenv
from app.models.vector_models import (
    AddVectorRequest,
    SearchResult,
    VectorSearchRequest,
    VectorSearchResponse
)
from fastapi.responses import JSONResponse

load_dotenv()

api_key = os.environ.get("GOOGLE_API_KEY")
os.environ["GOOGLE_API_KEY"] = api_key

router = APIRouter()
logger = logging.getLogger(__name__)

def standardization(distance: float) -> float:
    """Chuy·ªÉn ƒë·ªïi kho·∫£ng c√°ch L2 th√†nh ƒëi·ªÉm t∆∞∆°ng ƒë·ªìng (similarity score) trong kho·∫£ng [0, 1]."""
    if distance < 0:
        return 0.0
    else:
        return 1 / (1 + distance)

@router.post("/add", response_model=dict)
async def add_vector_document(
    file: UploadFile = File(...),
    uploaded_by: str = Form(...),
    current_user: dict = Depends(verify_token_v2)
):
    try:
        file_name = file.filename
        
        # Check for duplicate filename
        file_path, vector_db_path = get_file_paths(file_name)
        if os.path.exists(file_path):
            raise HTTPException(
                status_code=409, 
                detail=f"File already exists at path: {file_path}"
            )
        
        # Validate file extension
        supported_extensions = {'.pdf', '.txt', '.docx', '.csv', '.xlsx', '.xls'}
        file_extension = os.path.splitext(file_name.lower())[1]
        if file_extension not in supported_extensions:
            raise HTTPException(status_code=400, detail=f"File format {file_extension} not supported")
        
        # Generate unique ID and metadata
        generated_id = str(uuid.uuid4())
        vietnam_tz = timezone(timedelta(hours=7))
        created_at = datetime.now(vietnam_tz).isoformat()
        
        file_path, vector_db_path = get_file_paths(file_name)
        file_url = file_path
        
        metadata = AddVectorRequest(
            _id=generated_id,
            filename=file_name,
            url=file_url,
            uploaded_by=uploaded_by,
            createdAt=created_at
        )
        
        # Save file to disk
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        # Save metadata and add to embedding
        try:
            save_metadata(metadata)
            add_to_embedding(file_path, metadata)
        except Exception as embed_error:
            # Clean up file if metadata/embedding fails
            if os.path.exists(file_path):
                os.remove(file_path)
            raise HTTPException(
                status_code=500, 
                detail=f"Failed to process embeddings: {str(embed_error)}"
            )
        
        return {
            "message": "Vector added successfully",
            "_id": generated_id,
            "filename": file_name,
            "file_path": file_path,
            "vector_db_path": vector_db_path,
            "status": "created"
        }
        
    except HTTPException:
        raise
    except json.JSONDecodeError as json_error:
        raise HTTPException(status_code=400, detail=f"Invalid JSON in role fields: {str(json_error)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")

@router.delete("/{doc_id}", response_model=dict)
async def delete_vector_document(
    doc_id: str,
    current_user: dict = Depends(verify_token_v2)
):
    try:
        doc_info = find_document_info(doc_id)
        if not doc_info:
            raise HTTPException(status_code=404, detail=f"Document with ID {doc_id} not found")
        
        filename = doc_info.get('filename')
        file_path = doc_info.get('url')
        
        _, vector_db_path = get_file_paths(filename)
        
        deletion_results = {
            "file_deleted": False,
            "metadata_deleted": False,
            "vector_deleted": False
        }
        
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            deletion_results["file_deleted"] = True
        
        deletion_results["vector_deleted"] = delete_from_faiss_index(vector_db_path, doc_id)
        deletion_results["metadata_deleted"] = delete_metadata(doc_id)
        
        message = "Document deleted successfully" if all(deletion_results.values()) else "Document partially deleted"
        response = {
            "message": message,
            "_id": doc_id,
            "filename": filename,
            "deletion_results": deletion_results
        }
        
        if not all(deletion_results.values()):
            response["warning"] = "Some components could not be deleted"
        
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting document: {str(e)}")

@router.get("/{doc_id}", response_model=dict)
async def get_vector_document(
    doc_id: str,
    current_user: dict = Depends(verify_token_v2)
):
    try:
        doc_info = find_document_info(doc_id)
        if not doc_info:
            raise HTTPException(status_code=404, detail=f"Document with ID {doc_id} not found")
        
        file_path = doc_info.get('url')
        file_exists = os.path.exists(file_path) if file_path else False
        
        _, vector_db_path = get_file_paths(doc_info.get('filename'))
        vector_exists = os.path.exists(f"{vector_db_path}/index.faiss") and os.path.exists(f"{vector_db_path}/index.pkl")
        
        file_size = os.path.getsize(file_path) if file_exists else None
        
        return {
            **doc_info,
            "file_exists": file_exists,
            "vector_exists": vector_exists,
            "file_size": file_size
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error getting document: {str(e)}")

@router.put("/{doc_id}", response_model=dict)
async def update_vector_document(
    doc_id: str,
    current_user: dict = Depends(verify_token_v2),
    filename: str = Form(None),
    uploaded_by: str = Form(None),
    force_re_embed: bool = Form(False)
):
    try:
        current_doc = find_document_info(doc_id)
        if not current_doc:
            raise HTTPException(status_code=404, detail=f"Document with ID {doc_id} not found")
        
        old_metadata = current_doc.copy()
        current_filename = current_doc.get('filename')
        current_file_path = current_doc.get('url')
        
        # Handle filename validation and extension protection
        final_filename = current_filename
        if filename:
            current_name, current_extension = os.path.splitext(current_filename)
            input_name, input_extension = os.path.splitext(filename)
            
            if input_extension:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Please provide filename without extension. Current file extension '{current_extension}' will be preserved automatically."
                )
            
            final_filename = filename + current_extension
            
            supported_extensions = {'.pdf', '.txt', '.docx', '.csv', '.xlsx', '.xls'}
            if current_extension.lower() not in supported_extensions:
                raise HTTPException(
                    status_code=400, 
                    detail=f"Current file extension '{current_extension}' is not supported"
                )
        
        # Check for duplicate filename if filename is being changed
        if filename and final_filename != current_filename:
            target_file_path, _ = get_file_paths(final_filename)
            if os.path.exists(target_file_path):
                raise HTTPException(
                    status_code=409,
                    detail=f"File '{final_filename}' already exists at path: {target_file_path}"
                )
        
        new_filename = final_filename
        new_uploaded_by = uploaded_by or current_doc.get('uploaded_by')
        
        filename_changed = filename and new_filename != current_filename
        
        operations = {
            "file_renamed": False,
            "vector_updated": False,
            "metadata_updated": False,
            "update_method": "none"
        }
        
        final_file_path = current_file_path
        if filename_changed:
            new_file_path, _ = get_file_paths(new_filename)
            if os.path.exists(current_file_path):
                os.makedirs(os.path.dirname(new_file_path), exist_ok=True)
                shutil.move(current_file_path, new_file_path)
                operations["file_renamed"] = True
                final_file_path = new_file_path
        
        new_metadata = AddVectorRequest(
            _id=doc_id,
            filename=new_filename,
            url=final_file_path,
            uploaded_by=new_uploaded_by,
            createdAt=current_doc.get('createdAt')
        )
        
        operations["vector_updated"] = smart_metadata_update(doc_id, old_metadata, new_metadata, force_re_embed)
        operations["update_method"] = "full_re_embed" if (filename_changed or force_re_embed) else "metadata_only"
        
        delete_metadata(doc_id)
        save_metadata(new_metadata)
        operations["metadata_updated"] = True
        
        response = {
            "message": "Document updated successfully" if operations["vector_updated"] and operations["metadata_updated"] else "Document partially updated",
            "_id": doc_id,
            "success": operations["vector_updated"] and operations["metadata_updated"],
            "updated_fields": {
                "filename": {"old": current_filename, "new": new_filename, "changed": filename_changed},
                "uploaded_by": {"old": current_doc.get('uploaded_by'), "new": new_uploaded_by, "changed": new_uploaded_by != current_doc.get('uploaded_by')},
            },
            "operations": operations,
            "paths": {
                "old_file_path": current_file_path,
                "new_file_path": final_file_path,
                "old_vector_db": get_file_paths(current_filename)[1],
                "new_vector_db": get_file_paths(new_filename)[1]
            },
            "updatedAt": datetime.now(timezone(timedelta(hours=7))).isoformat(),
            "force_re_embed": force_re_embed
        }
        
        if not operations["vector_updated"] or not operations["metadata_updated"]:
            response["warnings"] = []
            if not operations["vector_updated"]:
                response["warnings"].append("Vector embeddings update failed")
            if not operations["metadata_updated"]:
                response["warnings"].append("Metadata database update failed")
        
        return response
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error updating document: {str(e)}")

@router.post("/search", response_model=VectorSearchResponse)
async def search_vector_documents(
    request: VectorSearchRequest,
    current_user: dict = Depends(verify_token_v2) 
):
    start_time = time.time()
    
    try:
        _, vector_db_path = get_file_paths("dummy_filename")
        
        # Check if vector DB exists
        if not (os.path.exists(f"{vector_db_path}/index.faiss") and os.path.exists(f"{vector_db_path}/index.pkl")):
            return VectorSearchResponse(
                query=request.query,
                results=[],
                total_found=0,
                k_requested=request.k,
                similarity_threshold=request.similarity_threshold,
                search_time_ms=round((time.time() - start_time) * 1000, 2)
            )
        
        # Use consistent embedding model
        try:
            embedding_model = get_embedding_model()
            db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load vector database: {str(e)}")
        
        # Perform similarity search with scores
        try:
            docs_with_scores = db.similarity_search_with_score(
                request.query, 
                k=request.k 
            )
            
            # Convert L2 distance to similarity score
            standardization_docs = [
                (doc, standardization(score)) for doc, score in docs_with_scores 
            ]

            filtered_docs = [
                (doc, score) for doc, score in standardization_docs
                if score >= request.similarity_threshold  
            ]
            
            # Convert to SearchResult format
            search_results = [
                {
                    "content": doc.page_content,
                    "metadata": {**doc.metadata, "similarity_score": float(score)}
                }
                for doc, score in filtered_docs
            ]
            
            top_results = search_results[:request.k]
            
            results = [
                SearchResult(
                    content=result["content"], 
                    metadata=result["metadata"]
                )
                for result in top_results
            ]
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Search execution failed: {str(e)}")
        
        search_time_ms = round((time.time() - start_time) * 1000, 2)
        return VectorSearchResponse(
            query=request.query,
            results=results,
            total_found=len(results),
            k_requested=request.k,
            similarity_threshold=request.similarity_threshold,
            search_time_ms=search_time_ms
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail="Internal server error")

@router.post("/search-with-llm")
async def search_with_llm(
    request: VectorSearchRequest,
    current_user: dict = Depends(verify_token_v2)
):
    start_time = time.time()

    try:
        _, vector_db_path = get_file_paths("dummy_filename")

        # Check if vector DB exists
        if not (os.path.exists(f"{vector_db_path}/index.faiss") and os.path.exists(f"{vector_db_path}/index.pkl")):
            return {"llm_response": "No relevant information found in the provided documents."}

        # Load vector DB
        try:
            embedding_model = get_embedding_model()
            db = FAISS.load_local(vector_db_path, embedding_model, allow_dangerous_deserialization=True)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to load vector database: {str(e)}")

        # Perform similarity search
        try:
            docs_with_scores = db.similarity_search_with_score(
                request.query,
                k=request.k
            )

            # Chu·∫©n h√≥a v√† l·ªçc theo threshold
            filtered_docs = [
                (doc, standardization(score)) for doc, score in docs_with_scores
                if standardization(score) >= request.similarity_threshold
            ]

            # Chuy·ªÉn sang dict
            search_results = [
                {
                    "content": doc.page_content,
                    "metadata": {**doc.metadata, "similarity_score": float(score)}
                }
                for doc, score in filtered_docs
            ]

            top_results = search_results[:request.k]

            # Generate LLM response
            llm_response = "No relevant information found in the provided documents."
            if top_results:
                try:
                    llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash")

                    context = "\n\n".join(
                        [f"Document {i+1}:\n{result['content']}" for i, result in enumerate(top_results)]
                    )

                    prompt_template = PromptTemplate(
                        input_variables=["query", "context"],
                        template="""
üéØ Vai tr√≤:
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n nghi·ªáp, ch·ªâ tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin t·ª´ **t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p**.

üìã Nguy√™n t·∫Øc:
- Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin t·ª´ t√†i li·ªáu
- Kh√¥ng th√™m ki·∫øn th·ª©c b√™n ngo√†i
- Kh√¥ng suy ƒëo√°n ho·∫∑c gi·∫£ ƒë·ªãnh
- N·∫øu kh√¥ng c√≥ th√¥ng tin: "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan trong t√†i li·ªáu."

üìù C·∫•u tr√∫c tr·∫£ l·ªùi:
1. **C√¢u m·ªü ƒë·∫ßu**: T√≥m t·∫Øt ng·∫Øn g·ªçn (1-2 c√¢u)
2. **N·ªôi dung ch√≠nh**: Tr√¨nh b√†y b·∫±ng danh s√°ch c√≥ s·ªë th·ª© t·ª± ho·∫∑c g·∫°ch ƒë·∫ßu d√≤ng
3. **K·∫øt lu·∫≠n** (n·∫øu c·∫ßn): T√≥m l∆∞·ª£c ho·∫∑c l·ªùi khuy√™n

üí° Format markdown:
- D√πng **s·ªë th·ª© t·ª±** (1., 2., 3.) cho c√°c b∆∞·ªõc ho·∫∑c quy tr√¨nh
- D√πng **g·∫°ch ƒë·∫ßu d√≤ng** (-, *, ‚Ä¢) cho danh s√°ch c√°c √Ω
- D√πng **bold** cho t·ª´ kh√≥a quan tr·ªçng
- D√πng > cho tr√≠ch d·∫´n t·ª´ t√†i li·ªáu (n·∫øu c·∫ßn)

‚ùì C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng:
{query}

üìÇ T√†i li·ªáu tham kh·∫£o:
{context}

H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n t√†i li·ªáu tr√™n.
"""
                    )

                    prompt = prompt_template.format(query=request.query, context=context)
                    llm_response = llm.invoke(prompt).content

                except Exception as e:
                    logger.error(f"LLM response generation failed: {str(e)}")
                    llm_response = "Failed to generate LLM response."

            return {"llm_response": llm_response}

        except Exception as e:
            logger.error(f"Search execution failed: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"Search execution failed: {str(e)}")

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail="Internal server error")

class ProcessQueryRequest(BaseModel):
    query: str
    thread_id: str | None = None

class ProcessQueryResponse(BaseModel):
    status: str
    data: dict | None
    error: str | None
    thread_id: str | None

# ================== API ROUTE ==================
@router.post("/process-query", response_model=ProcessQueryResponse)  
async def process_query_api(
    request: ProcessQueryRequest,
):
    start_time = time.time()
    try:
        result_json = process_query(request.query, thread_id=request.thread_id)
        
        # Parse JSON string th√†nh dict ƒë·ªÉ validate v·ªõi Pydantic
        result_dict = json.loads(result_json)
        
        # T·∫°o instance Pydantic model t·ª´ dict (s·∫Ω validate data)
        try:
            validated_response = ProcessQueryResponse(**result_dict)
            return validated_response
        except ValueError as ve:  # Pydantic validation error
            logger.warning(f"Pydantic validation failed: {str(ve)}. Fallback to raw JSON.")
            # Fallback: Tr·∫£ v·ªÅ raw JSON m√† kh√¥ng strict model
            return JSONResponse(
                content=result_dict,
                status_code=200,
                media_type="application/json"
            )
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"L·ªói khi x·ª≠ l√Ω query: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")
    finally:
        elapsed = time.time() - start_time
        logger.info(f"X·ª≠ l√Ω query m·∫•t {elapsed:.2f}s")